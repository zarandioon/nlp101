{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.utils import normalizeRows, softmax\n",
    "from word2vec import getNegativeSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE (~1 Line)\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n",
    "\n",
    "word2vec.sigmoid = sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from word2vec import sigmoid\n",
    "sigmoid(np.zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models.\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n",
    "                      (U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### Please use the provided softmax function (imported earlier in this file)\n",
    "    ### This numerically stable implementation helps you avoid issues pertaining\n",
    "    ### to integer overflow.\n",
    "    y_hat = softmax(np.dot(centerWordVec, outsideVectors.T))\n",
    "    delta = y_hat.copy()\n",
    "    delta[outsideWordIdx] -= 1\n",
    "\n",
    "    loss = -np.log(y_hat)[outsideWordIdx]\n",
    "    gradCenterVec = np.dot(delta, outsideVectors)\n",
    "    gradOutsideVecs = np.dot(delta[:, np.newaxis], centerWordVec[np.newaxis, :])\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "word2vec.naiveSoftmaxLossAndGradient = naiveSoftmaxLossAndGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "    gradCenterVec = np.zeros(centerWordVec.shape)\n",
    "    loss = 0.0\n",
    "    # allVecs = outsideVectors[indices]\n",
    "    # allScores = np.dot(allVecs, centerWordVec)\n",
    "\n",
    "    # loss = - np.log(sigmoid(allScores[0])) - np.sum(np.log(sigmoid(-allScores[1:])))\n",
    "    # # maybe the equation below is wrong\n",
    "    # gradCenterVec = (allScores[0] - 1) * allVecs[0] + (1 - sigmoid(-allScores[1:])).dot(allVecs[1:])\n",
    "\n",
    "    # gradOutsideVecs = np.zeros_like(outsideVectors)\n",
    "    # gradOutsideVecs[outsideWordIdx] -= (1 - sigmoid(allScores[0])) * centerWordVec\n",
    "    # for negSampleWordIdx, i in zip(negSampleWordIndices, range(1, len(indices))):\n",
    "    #     gradOutsideVecs[negSampleWordIdx] += (\n",
    "    #         1 - sigmoid(-allScores[i])) * allVecs[i]\n",
    "    z = sigmoid(np.dot(outsideVectors[outsideWordIdx], centerWordVec))\n",
    "    loss -= np.log(z)\n",
    "\n",
    "    gradOutsideVecs[outsideWordIdx] += centerWordVec * (z - 1.0)\n",
    "    gradCenterVec += outsideVectors[outsideWordIdx] * (z - 1.0)\n",
    "\n",
    "    # Implementation with for loop\n",
    "    #for k in range(K):\n",
    "    #    samp = indices[k+1]\n",
    "    #    z = sigmoid(np.dot(-outsideVectors[samp], centerWordVec))\n",
    "    #    loss -= np.log(z)\n",
    "    #    gradOutsideVecs[samp] -=  centerWordVec * (z - 1.0)\n",
    "    #    gradCenterVec -= outsideVectors[samp] * (z - 1.0)\n",
    "    # Vectorized implementation\n",
    "    u_k = outsideVectors[negSampleWordIndices]\n",
    "    z = sigmoid(-np.dot(u_k,centerWordVec))\n",
    "    loss += np.sum(- np.log(z))\n",
    "    gradCenterVec += np.dot((z-1),u_k)*(-1)\n",
    "    gradOutsideVecs[negSampleWordIndices] += np.outer((z-1),centerWordVec)*(-1)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "word2vec.negSamplingLossAndGradient = negSamplingLossAndGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "    Implement the skip-gram model in this function.\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) for all words in vocab\n",
    "                        (V in pdf handout)\n",
    "    outsideVectors -- outside word vectors (as rows) for all words in vocab\n",
    "                    (U in pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVecs -- the gradient with respect to the center word vectors\n",
    "            (dJ / dV in the pdf handout)\n",
    "    gradOutsideVectors -- the gradient with respect to the outside word vectors\n",
    "                        (dJ / dU in the pdf handout)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    currentCenterWordIdx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[currentCenterWordIdx]\n",
    "\n",
    "    for outsideWord in outsideWords:\n",
    "        outsideWordIdx = word2Ind[outsideWord]\n",
    "        (l, gradCenter, gradOutside) = word2vecLossAndGradient(\n",
    "            centerWordVec, outsideWordIdx, outsideVectors, dataset)\n",
    "        loss += l\n",
    "        gradCenterVecs[currentCenterWordIdx] += gradCenter\n",
    "        gradOutsideVectors += gradOutside\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "\n",
    "word2vec.skipgram = skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sgd import sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a loss and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    exploss = None\n",
    "\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        loss = None\n",
    "        ### YOUR CODE HERE (~2 lines)\n",
    "        loss, gradients = f(x)\n",
    "        x -= step * gradients\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = .95 * exploss + .05 * loss\n",
    "            print(\"iter %d: %f\" % (iter, exploss))\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "======Skip-Gram with naiveSoftmaxLossAndGradient Test Cases======\n",
      "The first test passed!\n",
      "The second test passed!\n",
      "The third test passed!\n",
      "All 3 tests passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Gradient check failed for negSamplingLossAndGradient Gradient.\n",
      "First gradient error found at index (5, 0) in the vector of gradients\n",
      "Your gradient: -1.258186 \t Numerical gradient: -3.398960\n",
      "======Skip-Gram with negSamplingLossAndGradient======\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Your gradOutsideVectors do not match expected gradOutsideVectors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-90f85069bf9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/nlp101/assignments/a2/word2vec.py\u001b[0m in \u001b[0;36mtest_word2vec\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mgrad_tests_negsamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegSamplingLossAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/nlp101/assignments/a2/utils/gradcheck.py\u001b[0m in \u001b[0;36mgrad_tests_negsamp\u001b[0;34m(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient)\u001b[0m\n\u001b[1;32m    159\u001b[0m            \u001b[0;34m\"Your gradCenterVecs do not match expected gradCenterVecs.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_gradOutsideVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_gradOutsideVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m            \u001b[0;34m\"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The first test passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Your gradOutsideVectors do not match expected gradOutsideVectors."
     ]
    }
   ],
   "source": [
    "from word2vec import test_word2vec\n",
    "test_word2vec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
